{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfac40d6",
   "metadata": {
    "id": "cfac40d6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e802052b",
   "metadata": {
    "id": "e802052b"
   },
   "outputs": [],
   "source": [
    "def randomForest():\n",
    "    bist100 = pd.read_csv(\"/content/drive/MyDrive/dp/SP500.csv\")\n",
    "    bist100.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\"},\n",
    "                   inplace=True)\n",
    "    bist100.isnull().sum()\n",
    "    bist100.isna().any()\n",
    "    bist100.dropna(inplace=True)\n",
    "    bist100.isna().any()\n",
    "    bist100['date'] = pd.to_datetime(bist100.date)\n",
    "    bist100.sort_values(by='date', inplace=True)\n",
    "    print(\"*******************************************************************************************\")\n",
    "    print(\"Starting date: \", bist100.iloc[0][0])\n",
    "    print(\"Ending date: \", bist100.iloc[-1][0])\n",
    "    print(\"Duration: \", bist100.iloc[-1][0] - bist100.iloc[0][0])\n",
    "    bist100.groupby(bist100['date'].dt.strftime('%B'))['low'].min()\n",
    "    closedf = bist100[['date', 'close']]\n",
    "    close_stock = closedf.copy()\n",
    "    del closedf['date']\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    closedf = scaler.fit_transform(np.array(closedf))\n",
    "    training_size = int(len(closedf) * 0.65)\n",
    "    test_size = len(closedf) - training_size\n",
    "    train_data, test_data = closedf[0:test_size, :], closedf[test_size:len(closedf), :]\n",
    "    def create_dataset(dataset, time_step=5):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset) - time_step - 1):\n",
    "            a = dataset[i:(i + time_step), 0] \n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    time_step = 10\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    regressor = RandomForestRegressor(max_depth=1)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    train_predict = regressor.predict(X_train)\n",
    "    test_predict = regressor.predict(X_test)\n",
    "    train_predict = train_predict.reshape(-1, 1)\n",
    "    test_predict = test_predict.reshape(-1, 1)\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    original_ytrain = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    original_ytest = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    print(\"Random forest MSE: \", mean_squared_error(original_ytest, test_predict))\n",
    "    print(\"*******************************************************************************************\\n\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bTKvHDCOKyjt",
   "metadata": {
    "id": "bTKvHDCOKyjt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from math import pi,sqrt,exp,pow,log\n",
    "from numpy.linalg import det, inv\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn import cluster\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "import scipy.optimize as sco\n",
    "import scipy.interpolate as sci\n",
    "from scipy import stats\n",
    "\n",
    "def singleInputLSTM():\n",
    "    split = (0.65)\n",
    "    sequence_length = 10;\n",
    "    normalise = True\n",
    "    batch_size = 100;\n",
    "    input_dim = 1\n",
    "    input_timesteps = 9\n",
    "    neurons = 50\n",
    "    epochs = 5\n",
    "    prediction_len = 1\n",
    "    dense_output = 1\n",
    "    drop_out = 0\n",
    "    dataframe = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "    cols = ['Adj Close']\n",
    "    len_dataframe = dataframe.shape[0]\n",
    "    i_split = int(len(dataframe) * split)\n",
    "    data_train = dataframe.get(cols).values[:i_split]\n",
    "    data_test = dataframe.get(cols).values[i_split:]\n",
    "    len_train = len(data_train)\n",
    "    len_test = len(data_test)\n",
    "    len_train_windows = None\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_test - sequence_length):\n",
    "        data_windows.append(data_test[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    y_test_ori = data_windows[:, -1, [0]]\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2] \n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num): \n",
    "        normalised_window = []\n",
    "        for col_i in range(0, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min)\n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)\n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "\n",
    "    data_windows = normalised_data \n",
    "    x_test = data_windows[:, :-1]\n",
    "    y_test = data_windows[:, -1, [0]]\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    for win_i in range(0, win_num): \n",
    "        normalised_window = []\n",
    "        for col_i in range(0, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train = data_windows[:, :-1]\n",
    "    y_train = data_windows[:, -1, [0]]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=True))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(LSTM(neurons, return_sequences=True))\n",
    "    model.add(LSTM(neurons, return_sequences=False))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(dense_output, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    model.predict(x_test)\n",
    "\n",
    "    data = x_test\n",
    "    prediction_seqs = []\n",
    "    window_size = sequence_length\n",
    "    pre_win_num = int(len(data) / prediction_len)\n",
    "    for i in range(0, pre_win_num): \n",
    "        curr_frame = data[i * prediction_len]\n",
    "        predicted = []\n",
    "        for j in range(0, prediction_len):\n",
    "            temp = model.predict(curr_frame[newaxis, :, :])[0]\n",
    "            predicted.append(temp)\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
    "        prediction_seqs.append(predicted)\n",
    "    de_predicted = []\n",
    "    len_pre_win = int(len(data) / prediction_len)\n",
    "    len_pre = prediction_len\n",
    "    m = 0\n",
    "    for i in range(0, len_pre_win):\n",
    "        for j in range(0, len_pre):\n",
    "            de_predicted.append(prediction_seqs[i][j][0] * record_max[m] + record_min[m])\n",
    "            m = m + 1\n",
    "    error = []\n",
    "    diff = y_test.shape[0] - prediction_len * pre_win_num\n",
    "    for i in range(y_test_ori.shape[0] - diff):\n",
    "        error.append(y_test_ori[i,] - de_predicted[i])\n",
    "    squaredError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)\n",
    "\n",
    "    MSE = sum(squaredError) / len(squaredError)\n",
    "    print(\"*****************************************************************************************************\")\n",
    "    print(\"LSTM-MSE:{}\".format(MSE))\n",
    "    print(\"*****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cXKOsFMuFE1j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXKOsFMuFE1j",
    "outputId": "2a7dd02b-52c8-4b54-c7c9-bea85e282acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************************\n",
      "Starting date:  2017-12-07 00:00:00\n",
      "Ending date:  2018-06-01 00:00:00\n",
      "Duration:  176 days 00:00:00\n",
      "Random forest MSE:  2546.2569412935563\n",
      "*******************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.4997\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4672\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.4353\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4032\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.3704\n",
      "2/2 [==============================] - 4s 14ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "*****************************************************************************************************\n",
      "LSTM-MSE:[2121.45161221]\n",
      "*****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "randomForest()\n",
    "singleInputLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "URH1pOWlhqLE",
   "metadata": {
    "id": "URH1pOWlhqLE"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "def gridSearch():\n",
    "    bist100 = pd.read_csv(\"/content/drive/MyDrive/dp/SP500.csv\")\n",
    "    bist100.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\"},\n",
    "                   inplace=True)\n",
    "    bist100.isnull().sum()\n",
    "    bist100.isna().any()\n",
    "    bist100.dropna(inplace=True)\n",
    "    bist100.isna().any()\n",
    "    bist100['date'] = pd.to_datetime(bist100.date)\n",
    "    bist100.sort_values(by='date', inplace=True)\n",
    "    bist100.groupby(bist100['date'].dt.strftime('%B'))['low'].min()\n",
    "    closedf = bist100[['date', 'close']]\n",
    "    close_stock = closedf.copy()\n",
    "    del closedf['date']\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    closedf = scaler.fit_transform(np.array(closedf))\n",
    "    \n",
    "    training_size = int(len(closedf) * 0.65)\n",
    "    test_size = len(closedf) - training_size\n",
    "    train_data, test_data = closedf[0:test_size, :], closedf[test_size:len(closedf), :]\n",
    "    def create_dataset(dataset, time_step=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset) - time_step - 1):\n",
    "            a = dataset[i:(i + time_step), 0]  \n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    time_step = 10\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_test, y_test = create_dataset(test_data, time_step)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    param_test1 = [{'n_estimators':[50,120,160,200,250]},{'max_depth':[1,2,3,5,7,9,11,13]},{'min_samples_split':[100,120,150,180,200,300]}]\n",
    "    gsearch1 = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_test1, cv=5)\n",
    "    gsearch1.fit(X_train,y_train)\n",
    "    print(\"*******************************************************************************************\")\n",
    "    print( \"By gridSearch The best model is :\",gsearch1.best_estimator_)\n",
    "    print(\"*******************************************************************************************\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "RLP0aKVlS0KR",
   "metadata": {
    "id": "RLP0aKVlS0KR"
   },
   "outputs": [],
   "source": [
    "def Opt_randomForest():\n",
    "    bist100 = pd.read_csv(\"/content/drive/MyDrive/dp/SP500.csv\")\n",
    "    bist100.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\"},\n",
    "                   inplace=True)\n",
    "    bist100.isnull().sum()\n",
    "    bist100.isna().any()\n",
    "    bist100.dropna(inplace=True)\n",
    "    bist100.isna().any()\n",
    "    bist100['date'] = pd.to_datetime(bist100.date)\n",
    "    bist100.sort_values(by='date', inplace=True)\n",
    "    print(\"*******************************************************************************************\")\n",
    "    print(\"Starting date: \", bist100.iloc[0][0])\n",
    "    print(\"Ending date: \", bist100.iloc[-1][0])\n",
    "    print(\"Duration: \", bist100.iloc[-1][0] - bist100.iloc[0][0])\n",
    "    bist100.groupby(bist100['date'].dt.strftime('%B'))['low'].min()\n",
    "    closedf = bist100[['date', 'close']]\n",
    "    close_stock = closedf.copy()\n",
    "    del closedf['date']\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    closedf = scaler.fit_transform(np.array(closedf))\n",
    "    training_size = int(len(closedf) * 0.65)\n",
    "    test_size = len(closedf) - training_size\n",
    "    train_data, test_data = closedf[0:test_size, :], closedf[test_size:len(closedf), :]\n",
    "    def create_dataset(dataset, time_step=5):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset) - time_step - 1):\n",
    "            a = dataset[i:(i + time_step), 0] \n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + time_step, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "\n",
    "    time_step = 10\n",
    "    X_train, y_train = create_dataset(train_data, time_step)\n",
    "    X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    regressor = RandomForestRegressor(max_depth=11)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    train_predict = regressor.predict(X_train)\n",
    "    test_predict = regressor.predict(X_test)\n",
    "    train_predict = train_predict.reshape(-1, 1)\n",
    "    test_predict = test_predict.reshape(-1, 1)\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    original_ytrain = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    original_ytest = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    print(\"Random forest algorithm after grid search optimization MSE: \", mean_squared_error(original_ytest, test_predict))\n",
    "    print(\"*******************************************************************************************\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "EfaB3qFzXPMi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfaB3qFzXPMi",
    "outputId": "4379e33c-24af-4915-d45a-6cab1a048189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************************\n",
      "By grindSearch The best model is : RandomForestRegressor(max_depth=11)\n",
      "*******************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gridSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ccdff5",
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2ccdff5",
    "outputId": "0e8e3f95-582e-4b1f-d6b6-9ff7068057b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************************\n",
      "Starting date:  2017-12-07 00:00:00\n",
      "Ending date:  2018-06-01 00:00:00\n",
      "Duration:  176 days 00:00:00\n",
      "Random forest MSE:  2439.9474727924794\n",
      "*******************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*******************************************************************************************\n",
      "Starting date:  2017-12-07 00:00:00\n",
      "Ending date:  2018-06-01 00:00:00\n",
      "Duration:  176 days 00:00:00\n",
      "Random forest algorithm after grid search optimization MSE:  1684.2587625453582\n",
      "*******************************************************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "randomForest()\n",
    "\n",
    "Opt_randomForest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wcW6nT1Xqs79",
   "metadata": {
    "id": "wcW6nT1Xqs79"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from math import pi,sqrt,exp,pow,log\n",
    "from numpy.linalg import det, inv\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn import cluster\n",
    "import statsmodels.api as sm \n",
    "import scipy.stats as scs\n",
    "import scipy.optimize as sco\n",
    "import scipy.interpolate as sci\n",
    "from scipy import stats\n",
    "def add_noise():\n",
    "  df = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "\n",
    "  wsj_var=np.var(df.wsj_mean_compound)\n",
    "  cnbc_var=np.var(df.cnbc_mean_compound)\n",
    "  fortune_var=np.var(df.fortune_mean_compound)\n",
    "  reuters_var=np.var(df.reuters_mean_compound)\n",
    "\n",
    "  mu=0\n",
    "  sigma_wsj=0.1*wsj_var\n",
    "  sigma_cnbc=0.1*cnbc_var\n",
    "  sigma_fortune=0.1*fortune_var\n",
    "  sigma_reuters=0.1*reuters_var\n",
    "  n=df.shape[0]\n",
    "  df_noise=pd.DataFrame()\n",
    "  df_noise['wsj_noise']=df['wsj_mean_compound']\n",
    "  df_noise['cnbc_noise']=df['cnbc_mean_compound']\n",
    "  df_noise['fortune_noise']=df['fortune_mean_compound']\n",
    "  df_noise['reuters_noise']=df['reuters_mean_compound']\n",
    "  for i in range(0,n):\n",
    "    df_noise['wsj_noise'][i]+=np.random.normal(mu,sigma_wsj)\n",
    "    df_noise['cnbc_noise'][i]+=np.random.normal(mu,sigma_cnbc)\n",
    "    df_noise['fortune_noise'][i]+=np.random.normal(mu,sigma_fortune)\n",
    "    df_noise['reuters_noise'][i]+=np.random.normal(mu,sigma_reuters)\n",
    "  df_noise.to_csv(\"/content/drive/MyDrive/dp/source_price_noise.csv\")\n",
    "  print(\"*****************************************************************************************************\")\n",
    "  print(\"Gaussian noise was successfully added to the data set\")\n",
    "  print(\"*****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ruFAecIxoGIN",
   "metadata": {
    "id": "ruFAecIxoGIN"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from math import pi,sqrt,exp,pow,log\n",
    "from numpy.linalg import det, inv\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn import cluster\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "import scipy.optimize as sco\n",
    "import scipy.interpolate as sci\n",
    "from scipy import stats\n",
    "def twoWeekSentimentLSTM():\n",
    "    df = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "    dfn = pd.read_csv(\"/content/drive/MyDrive/dp/source_price_noise.csv\", index_col=0)\n",
    "\n",
    "    df_1n = pd.DataFrame()\n",
    "    df_1n['wsj'] = dfn['wsj_noise']\n",
    "    df_1n['cnbc'] = df['cnbc_mean_compound']\n",
    "    df_1n['fortune'] = df['fortune_mean_compound']\n",
    "    df_1n['reuters'] = df['reuters_mean_compound']\n",
    "    df_1n['price'] = df['Adj Close']\n",
    "\n",
    "    df_2n = pd.DataFrame()\n",
    "    df_2n['wsj'] = df['wsj_mean_compound']\n",
    "    df_2n['cnbc'] = dfn['cnbc_noise']\n",
    "    df_2n['fortune'] = df['fortune_mean_compound']\n",
    "    df_2n['reuters'] = df['reuters_mean_compound']\n",
    "    df_2n['price'] = df['Adj Close']\n",
    "\n",
    "    df_3n = pd.DataFrame()\n",
    "    df_3n['wsj'] = df['wsj_mean_compound']\n",
    "    df_3n['cnbc'] = df['cnbc_mean_compound']\n",
    "    df_3n['fortune'] = dfn['fortune_noise']\n",
    "    df_3n['reuters'] = df['reuters_mean_compound']\n",
    "    df_3n['price'] = df['Adj Close']\n",
    "\n",
    "    df_4n = pd.DataFrame()\n",
    "    df_4n['wsj'] = df['wsj_mean_compound']\n",
    "    df_4n['cnbc'] = df['cnbc_mean_compound']\n",
    "    df_4n['fortune'] = df['fortune_mean_compound']\n",
    "    df_4n['reuters'] = dfn['reuters_noise']\n",
    "    df_4n['price'] = df['Adj Close']\n",
    "\n",
    "    df1 = df_1n\n",
    "    df2 = df_2n\n",
    "    df3 = df_3n\n",
    "    df4 = df_4n\n",
    "\n",
    "    split = (0.65)\n",
    "    sequence_length = 10;\n",
    "    normalise = True\n",
    "    batch_size = 100;\n",
    "    input_dim = 5\n",
    "    input_timesteps = 9\n",
    "    neurons = 50\n",
    "    epochs = 5\n",
    "    prediction_len = 1\n",
    "    dense_output = 1\n",
    "    drop_out = 0\n",
    "    i_split = int(len(df1) * split)\n",
    "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
    "    data_train_1 = df1.get(cols).values[:i_split]\n",
    "    data_train_2 = df2.get(cols).values[:i_split]\n",
    "    data_train_3 = df3.get(cols).values[:i_split]\n",
    "    data_train_4 = df4.get(cols).values[:i_split]\n",
    "    len_train = len(data_train_1)\n",
    "    len_train_windows = None\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_1[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max) \n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train1 = data_windows[:, :-1]\n",
    "    y_train1 = data_windows[:, -1, [0]]\n",
    "\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_2[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max) \n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train2 = data_windows[:, :-1]\n",
    "    y_train2 = data_windows[:, -1, [0]]\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_3[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min)  \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)\n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train3 = data_windows[:, :-1]\n",
    "    y_train3 = data_windows[:, -1, [0]]\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_4[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)\n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train4 = data_windows[:, :-1]\n",
    "    y_train4 = data_windows[:, -1, [0]]\n",
    "\n",
    "    x_train_t = np.concatenate((x_train1, x_train2, x_train3, x_train4), axis=0)\n",
    "    x_train = x_train_t\n",
    "    y_train_t = np.concatenate((y_train1, y_train2, y_train3, y_train4), axis=0)\n",
    "    y_train = y_train_t\n",
    "\n",
    "    dataframe = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "    dataframe.columns = ['date', 'wsj', 'cnbc', 'fortune', 'reuters', 'price']\n",
    "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
    "    len_dataframe = dataframe.shape[0]\n",
    "    i_split = int(len(dataframe) * split)\n",
    "    data_test = dataframe.get(cols).values[i_split:]\n",
    "    len_test = len(data_test)\n",
    "    len_train_windows = None\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_test - sequence_length):\n",
    "        data_windows.append(data_test[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    y_test_ori = data_windows[:, -1, [0]]\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "\n",
    "    # normalize\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max) \n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "\n",
    "    data_windows = normalised_data \n",
    "    x_test = data_windows[:, :-1]\n",
    "    y_test = data_windows[:, -1, [0]]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=True))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(LSTM(neurons, return_sequences=True))\n",
    "    model.add(LSTM(neurons, return_sequences=False))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(dense_output, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    data = x_test\n",
    "    prediction_seqs = []\n",
    "    window_size = sequence_length\n",
    "    pre_win_num = int(len(data) / prediction_len)\n",
    "    for i in range(0, pre_win_num):\n",
    "        curr_frame = data[i * prediction_len]\n",
    "        predicted = []\n",
    "        for j in range(0, prediction_len):\n",
    "            temp = model.predict(curr_frame[newaxis, :, :])[0]\n",
    "            predicted.append(temp)\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
    "        prediction_seqs.append(predicted)\n",
    "\n",
    "    de_predicted = []\n",
    "    len_pre_win = int(len(data) / prediction_len)\n",
    "    len_pre = prediction_len\n",
    "    m = 0\n",
    "    for i in range(0, len_pre_win):\n",
    "        for j in range(0, len_pre):\n",
    "            de_predicted.append(prediction_seqs[i][j][0] * record_max[m] + record_min[m])\n",
    "            m = m + 1\n",
    "    error = []\n",
    "    diff = y_test.shape[0] - prediction_len * pre_win_num\n",
    "    for i in range(y_test_ori.shape[0] - diff):\n",
    "        error.append(y_test_ori[i,] - de_predicted[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)\n",
    "\n",
    "    MSE = sum(squaredError) / len(squaredError)\n",
    "    print(\"*****************************************************************************************************\")\n",
    "    print(\"Sentimental-LSTM MSE:{}\".format(MSE))\n",
    "    print(\"*****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "UNic2EWYIWfk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UNic2EWYIWfk",
    "outputId": "c9967ff6-75cb-4b88-c2b4-e7b3e4ac11ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-27a8d268-b1f6-4a15-b78d-76cb59b9661a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wsj_mean_compound</th>\n",
       "      <th>cnbc_mean_compound</th>\n",
       "      <th>fortune_mean_compound</th>\n",
       "      <th>reuters_mean_compound</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017/12/7</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-0.1366</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2636.979980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017/12/8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2651.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017/12/11</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2659.989990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017/12/12</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2664.110107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017/12/13</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2662.850098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27a8d268-b1f6-4a15-b78d-76cb59b9661a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-27a8d268-b1f6-4a15-b78d-76cb59b9661a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-27a8d268-b1f6-4a15-b78d-76cb59b9661a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         date  wsj_mean_compound  cnbc_mean_compound  fortune_mean_compound  \\\n",
       "0   2017/12/7              0.296             -0.1366                 0.0000   \n",
       "1   2017/12/8              0.000              0.0000                -0.2423   \n",
       "2  2017/12/11              0.000              0.0000                 0.0000   \n",
       "3  2017/12/12              0.000              0.0000                 0.0000   \n",
       "4  2017/12/13              0.000              0.0000                 0.0000   \n",
       "\n",
       "   reuters_mean_compound    Adj Close  \n",
       "0                    0.0  2636.979980  \n",
       "1                    0.0  2651.500000  \n",
       "2                    0.0  2659.989990  \n",
       "3                    0.0  2664.110107  \n",
       "4                    0.0  2662.850098  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "VNHXBGRVG419",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNHXBGRVG419",
    "outputId": "3636bc9c-01af-41b2-8d37-b3f04c761d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************************************\n",
      "Gaussian noise was successfully added to the data set\n",
      "*****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "add_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "epmgDzVNH9Y8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epmgDzVNH9Y8",
    "outputId": "7df28c27-4b21-41c1-8816-4eaaefcd1b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.5312\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4975\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4667\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4377\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4093\n",
      "2/2 [==============================] - 1s 8ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "*****************************************************************************************************\n",
      "LSTM-MSE:[2429.24941014]\n",
      "*****************************************************************************************************\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 7s 28ms/step - loss: 0.4886\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.3926\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2987\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.2212\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2283\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "*****************************************************************************************************\n",
      "Sentimental-LSTM MSE:[811.75753353]\n",
      "*****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "singleInputLSTM()\n",
    "twoWeekSentimentLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Dv0yJY_lhtFr",
   "metadata": {
    "id": "Dv0yJY_lhtFr"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from numpy import newaxis\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from math import pi,sqrt,exp,pow,log\n",
    "from numpy.linalg import det, inv\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from sklearn import cluster\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "import scipy.optimize as sco\n",
    "import scipy.interpolate as sci\n",
    "from scipy import stats\n",
    "def oneWeekSentimentLSTM():\n",
    "    df = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "    dfn = pd.read_csv(\"/content/drive/MyDrive/dp/source_price_noise.csv\", index_col=0)\n",
    "\n",
    "    df_1n = pd.DataFrame()\n",
    "    df_1n['wsj'] = dfn['wsj_noise']\n",
    "    df_1n['cnbc'] = df['cnbc_mean_compound']\n",
    "    df_1n['fortune'] = df['fortune_mean_compound']\n",
    "    df_1n['reuters'] = df['reuters_mean_compound']\n",
    "    df_1n['price'] = df['Adj Close']\n",
    "\n",
    "    df_2n = pd.DataFrame()\n",
    "    df_2n['wsj'] = df['wsj_mean_compound']\n",
    "    df_2n['cnbc'] = dfn['cnbc_noise']\n",
    "    df_2n['fortune'] = df['fortune_mean_compound']\n",
    "    df_2n['reuters'] = df['reuters_mean_compound']\n",
    "    df_2n['price'] = df['Adj Close']\n",
    "\n",
    "    df_3n = pd.DataFrame()\n",
    "    df_3n['wsj'] = df['wsj_mean_compound']\n",
    "    df_3n['cnbc'] = df['cnbc_mean_compound']\n",
    "    df_3n['fortune'] = dfn['fortune_noise']\n",
    "    df_3n['reuters'] = df['reuters_mean_compound']\n",
    "    df_3n['price'] = df['Adj Close']\n",
    "\n",
    "    df_4n = pd.DataFrame()\n",
    "    df_4n['wsj'] = df['wsj_mean_compound']\n",
    "    df_4n['cnbc'] = df['cnbc_mean_compound']\n",
    "    df_4n['fortune'] = df['fortune_mean_compound']\n",
    "    df_4n['reuters'] = dfn['reuters_noise']\n",
    "    df_4n['price'] = df['Adj Close']\n",
    "\n",
    "    df1 = df_1n\n",
    "    df2 = df_2n\n",
    "    df3 = df_3n\n",
    "    df4 = df_4n\n",
    "\n",
    "    split = (0.65)\n",
    "    sequence_length = 5;\n",
    "    normalise = True\n",
    "    batch_size = 100;\n",
    "    input_dim = 5\n",
    "    input_timesteps = 4\n",
    "    neurons = 50\n",
    "    epochs = 5\n",
    "    prediction_len = 1\n",
    "    dense_output = 1\n",
    "    drop_out = 0\n",
    "\n",
    "    i_split = int(len(df1) * split)\n",
    "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
    "    data_train_1 = df1.get(cols).values[:i_split]\n",
    "    data_train_2 = df2.get(cols).values[:i_split]\n",
    "    data_train_3 = df3.get(cols).values[:i_split]\n",
    "    data_train_4 = df4.get(cols).values[:i_split]\n",
    "    len_train = len(data_train_1)\n",
    "    len_train_windows = None\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_1[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1):  \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)\n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train1 = data_windows[:, :-1]\n",
    "    y_train1 = data_windows[:, -1, [0]]\n",
    "\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_2[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max) \n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train2 = data_windows[:, :-1]\n",
    "    y_train2 = data_windows[:, -1, [0]]\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_3[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1):  \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min)  \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max) \n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train3 = data_windows[:, :-1]\n",
    "    y_train3 = data_windows[:, -1, [0]]\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_train - sequence_length):\n",
    "        data_windows.append(data_train_4[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1): \n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)\n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "    data_windows = normalised_data\n",
    "    x_train4 = data_windows[:, :-1]\n",
    "    y_train4 = data_windows[:, -1, [0]]\n",
    "\n",
    "    x_train_t = np.concatenate((x_train1, x_train2, x_train3, x_train4), axis=0)\n",
    "    x_train = x_train_t\n",
    "    y_train_t = np.concatenate((y_train1, y_train2, y_train3, y_train4), axis=0)\n",
    "    y_train = y_train_t\n",
    "\n",
    "    dataframe = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
    "    dataframe.columns = ['date', 'wsj', 'cnbc', 'fortune', 'reuters', 'price']\n",
    "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
    "    len_dataframe = dataframe.shape[0]\n",
    "    i_split = int(len(dataframe) * split)\n",
    "    data_test = dataframe.get(cols).values[i_split:]\n",
    "    len_test = len(data_test)\n",
    "    len_train_windows = None\n",
    "\n",
    "    data_windows = []\n",
    "    for i in range(len_test - sequence_length):\n",
    "        data_windows.append(data_test[i:i + sequence_length])\n",
    "    data_windows = np.array(data_windows).astype(float)\n",
    "    y_test_ori = data_windows[:, -1, [0]]\n",
    "\n",
    "    window_data = data_windows\n",
    "    win_num = window_data.shape[0]\n",
    "    col_num = window_data.shape[2]\n",
    "    normalised_data = []\n",
    "    record_min = []\n",
    "    record_max = []\n",
    "\n",
    "    for win_i in range(0, win_num):\n",
    "        normalised_window = []\n",
    "        for col_i in range(0, 1):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            temp_min = min(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_min.append(temp_min) \n",
    "            temp_col = temp_col - temp_min\n",
    "            temp_max = max(temp_col)\n",
    "            if col_i == 0:\n",
    "                record_max.append(temp_max)  \n",
    "            temp_col = temp_col / temp_max\n",
    "            normalised_window.append(temp_col)\n",
    "        for col_i in range(1, col_num):\n",
    "            temp_col = window_data[win_i, :, col_i]\n",
    "            normalised_window.append(temp_col)\n",
    "        normalised_window = np.array(normalised_window).T\n",
    "        normalised_data.append(normalised_window)\n",
    "    normalised_data = np.array(normalised_data)\n",
    "\n",
    "    data_windows = normalised_data \n",
    "    x_test = data_windows[:, :-1]\n",
    "    y_test = data_windows[:, -1, [0]]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=True))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(LSTM(neurons, return_sequences=True))\n",
    "    model.add(LSTM(neurons, return_sequences=False))\n",
    "    model.add(Dropout(drop_out))\n",
    "    model.add(Dense(dense_output, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer='adam')\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    data = x_test\n",
    "    prediction_seqs = []\n",
    "    window_size = sequence_length\n",
    "    pre_win_num = int(len(data) / prediction_len)\n",
    "    for i in range(0, pre_win_num):\n",
    "        curr_frame = data[i * prediction_len]\n",
    "        predicted = []\n",
    "        for j in range(0, prediction_len):\n",
    "            temp = model.predict(curr_frame[newaxis, :, :])[0]\n",
    "            predicted.append(temp)\n",
    "            curr_frame = curr_frame[1:]\n",
    "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
    "        prediction_seqs.append(predicted)\n",
    "\n",
    "    de_predicted = []\n",
    "    len_pre_win = int(len(data) / prediction_len)\n",
    "    len_pre = prediction_len\n",
    "    m = 0\n",
    "    for i in range(0, len_pre_win):\n",
    "        for j in range(0, len_pre):\n",
    "            de_predicted.append(prediction_seqs[i][j][0] * record_max[m] + record_min[m])\n",
    "            m = m + 1\n",
    "    error = []\n",
    "    diff = y_test.shape[0] - prediction_len * pre_win_num\n",
    "    for i in range(y_test_ori.shape[0] - diff):\n",
    "        error.append(y_test_ori[i,] - de_predicted[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)\n",
    "\n",
    "    MSE = sum(squaredError) / len(squaredError)\n",
    "    print(\"*****************************************************************************************************\")\n",
    "    print(\"one week Sentimental-LSTM MSE:{}\".format(MSE))\n",
    "    print(\"*****************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "DgmV_Hw5JO4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgmV_Hw5JO4e",
    "outputId": "14781cf9-88a1-4eb6-95de-487e26ce1b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 7s 29ms/step - loss: 0.4675\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.3699\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.2723\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.2211\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2284\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "*****************************************************************************************************\n",
      "Sentimental-LSTM MSE:[826.27159857]\n",
      "*****************************************************************************************************\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 8s 16ms/step - loss: 0.5012\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4610\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.4195\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3729\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.3205\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "*****************************************************************************************************\n",
      "one week Sentimental-LSTM MSE:[637.52295069]\n",
      "*****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "twoWeekSentimentLSTM()\n",
    "oneWeekSentimentLSTM()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensor1",
   "language": "python",
   "name": "tensor1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
