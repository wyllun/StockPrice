{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cfac40d6",
      "metadata": {
        "id": "cfac40d6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime as dt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
        "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e802052b",
      "metadata": {
        "id": "e802052b"
      },
      "outputs": [],
      "source": [
        "def randomForest():\n",
        "    # Import dataset\n",
        "    #The code reads a csv file named \"SP50.csv\" using pandas and stores it in a dataframe called \"bist100\".\n",
        "    bist100 = pd.read_csv(\"/content/drive/MyDrive/dp/SP500.csv\")\n",
        "    # Rename columns\n",
        "    bist100.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\"},\n",
        "                   inplace=True)\n",
        "    # Checking null value\n",
        "    bist100.isnull().sum()\n",
        "    # Checking na value\n",
        "    bist100.isna().any()\n",
        "    # The code drops any rows with null values using the \"dropna()\" method .\n",
        "    bist100.dropna(inplace=True)\n",
        "    #The code is used to check if there are any na values in the dataframe after dropping the null values.\n",
        "    bist100.isna().any()\n",
        "    # convert date field from string to Date format and make it index\n",
        "    bist100['date'] = pd.to_datetime(bist100.date)\n",
        "    # sorting dataset by date format\n",
        "    bist100.sort_values(by='date', inplace=True)\n",
        "    print(\"*******************************************************************************************\")\n",
        "    # Get the duration of dataset\n",
        "    # The time of the first bar of data\n",
        "    print(\"Starting date: \", bist100.iloc[0][0])\n",
        "    # Time of the last piece of data\n",
        "    print(\"Ending date: \", bist100.iloc[-1][0])\n",
        "    #duration\n",
        "    print(\"Duration: \", bist100.iloc[-1][0] - bist100.iloc[0][0])\n",
        "    # Monthwise High and Low stock price\n",
        "    bist100.groupby(bist100['date'].dt.strftime('%B'))['low'].min()\n",
        "    #Keep close date data\n",
        "    closedf = bist100[['date', 'close']]\n",
        "    #Make a copy of the data for easy use\n",
        "    close_stock = closedf.copy()\n",
        "    #Delete date, leaving only close\n",
        "    del closedf['date']\n",
        "    # Maximum minimization normalization\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    closedf = scaler.fit_transform(np.array(closedf))\n",
        "    # Training data 0.65 Test data 0.35\n",
        "    training_size = int(len(closedf) * 0.65)\n",
        "    test_size = len(closedf) - training_size\n",
        "    # Divide the data set according to the index divided above\n",
        "    train_data, test_data = closedf[0:test_size, :], closedf[test_size:len(closedf), :]\n",
        "    # Divide the data set according to the time window\n",
        "    # Using two weeks' worth of data to predict one day's worth of data\n",
        "    def create_dataset(dataset, time_step=5):\n",
        "        dataX, dataY = [], []\n",
        "        for i in range(len(dataset) - time_step - 1):\n",
        "            a = dataset[i:(i + time_step), 0]  ###i=0, 0,1,2,3------15\n",
        "            dataX.append(a)\n",
        "            dataY.append(dataset[i + time_step, 0])\n",
        "        return np.array(dataX), np.array(dataY)\n",
        "\n",
        "    # Using two weeks' worth of data to predict one day's worth of data\n",
        "    time_step = 10\n",
        "    X_train, y_train = create_dataset(train_data, time_step)\n",
        "    X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    # Build a random forest model\n",
        "    regressor = RandomForestRegressor(max_depth=1)\n",
        "    # Training model\n",
        "    regressor.fit(X_train, y_train)\n",
        "    # Lets Do the prediction\n",
        "    train_predict = regressor.predict(X_train)\n",
        "    test_predict = regressor.predict(X_test)\n",
        "    train_predict = train_predict.reshape(-1, 1)\n",
        "    test_predict = test_predict.reshape(-1, 1)\n",
        "    # From maximum to minimum normalization to its original form\n",
        "    train_predict = scaler.inverse_transform(train_predict)\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    original_ytrain = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "    original_ytest = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "    # Evaluation metrices MSE\n",
        "    print(\"Random forest MSE: \", mean_squared_error(original_ytest, test_predict))\n",
        "    print(\"*******************************************************************************************\\n\\n\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats\n",
        "\n",
        "def singleInputLSTM():# Both the training set and the test set are one-dimensional data, the stock price\n",
        "    # The training set size is 0.65\n",
        "    split = (0.65)\n",
        "    # The predicted sequence length is 10\n",
        "    sequence_length = 10;\n",
        "    # Data normalization\n",
        "    normalise = True\n",
        "    # Design the batch size of the neural network\n",
        "    batch_size = 100;\n",
        "    # The dimension of the input data is 5\n",
        "    input_dim = 1\n",
        "    # The length of input prediction data is 9, which means that the first 9 data are used to predict the 10th data\n",
        "    input_timesteps = 9\n",
        "    # Set the number of neurons for the neural network to 50\n",
        "    neurons = 50\n",
        "    # As the number of epochs increases, the number of weight update iterations increases, and the curve shifts from the initial unfitting state to the optimized fitting state\n",
        "    epochs = 5\n",
        "    # The number of predicted data at a time\n",
        "    prediction_len = 1\n",
        "    # dense layer Number of output data\n",
        "    dense_output = 1\n",
        "    # Sets the probability of data returning to zero\n",
        "    drop_out = 0\n",
        "    # Read the noiseless data for prediction\n",
        "    dataframe = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "    # Just use Adj Close for this column of elements\n",
        "    cols = ['Adj Close']\n",
        "    # Gets the number of rows in the table\n",
        "    len_dataframe = dataframe.shape[0]\n",
        "    # Gets the size of the test set and divides the data set\n",
        "    i_split = int(len(dataframe) * split)\n",
        "    data_train = dataframe.get(cols).values[:i_split]\n",
        "    data_test = dataframe.get(cols).values[i_split:]\n",
        "    len_train = len(data_train)\n",
        "    len_test = len(data_test)\n",
        "    len_train_windows = None\n",
        "\n",
        "    # Add a test set to the sliding window\n",
        "    data_windows = []\n",
        "    for i in range(len_test - sequence_length):\n",
        "        data_windows.append(data_test[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    # Save the initial test set for the final MSE calculation\n",
        "    y_test_ori = data_windows[:, -1, [0]]\n",
        "    # Create the data container required for maximum minimization\n",
        "    window_data = data_windows\n",
        "    # Statistics The number of sliding Windows\n",
        "    win_num = window_data.shape[0]\n",
        "    # Statistical column number\n",
        "    col_num = window_data.shape[2]\n",
        "    # Normalized data was recorded\n",
        "    normalised_data = []\n",
        "    # Record minimum values for calculation purposes\n",
        "    record_min = []\n",
        "    # Record the maximum value for calculation\n",
        "    record_max = []\n",
        "    # Maximum minimization\n",
        "    for win_i in range(0, win_num):  # Normalization is carried out for each sliding window\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        # Normalized data was recorded\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data  # get_test_data\n",
        "    # The normalized test set is obtained\n",
        "    x_test = data_windows[:, :-1]\n",
        "    y_test = data_windows[:, -1, [0]]\n",
        "\n",
        "    # Adds a training set to the sliding window\n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    # Create the data container required for maximum minimization\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    # Maximum minimization\n",
        "    normalised_data = []\n",
        "    for win_i in range(0, win_num):  # Normalization is carried out for each sliding window\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        # Normalized data was recorded\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    # The normalized test set is obtained\n",
        "    x_train = data_windows[:, :-1]\n",
        "    y_train = data_windows[:, -1, [0]]\n",
        "\n",
        "    # Create the LSTM model\n",
        "    model = Sequential()\n",
        "    # Build the LSTM hierarchy\n",
        "    model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=True))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(LSTM(neurons, return_sequences=True))\n",
        "    model.add(LSTM(neurons, return_sequences=False))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(Dense(dense_output, activation='linear'))\n",
        "    # Compile model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer='adam')\n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "    # Use models to make predictions\n",
        "    model.predict(x_test)\n",
        "\n",
        "    # multi sequence predict\n",
        "    data = x_test\n",
        "    prediction_seqs = []\n",
        "    # Each sliding window is predicted, and the predicted result of each sliding window is passed to the next sliding window\n",
        "    window_size = sequence_length\n",
        "    pre_win_num = int(len(data) / prediction_len)\n",
        "    for i in range(0, pre_win_num):  # \n",
        "        curr_frame = data[i * prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(0, prediction_len):\n",
        "            # Use models to make predictions\n",
        "            temp = model.predict(curr_frame[newaxis, :, :])[0]\n",
        "            # Save the prediction\n",
        "            predicted.append(temp)\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "    # Initializes the container and parameters used for de-normalization\n",
        "    de_predicted = []\n",
        "    len_pre_win = int(len(data) / prediction_len)\n",
        "    len_pre = prediction_len\n",
        "    # De-normalize the predicted data, that is, remove the maximum and minimum value normalization\n",
        "    m = 0\n",
        "    for i in range(0, len_pre_win):\n",
        "        for j in range(0, len_pre):\n",
        "            de_predicted.append(prediction_seqs[i][j][0] * record_max[m] + record_min[m])\n",
        "            m = m + 1\n",
        "    # Initialize the container and parameters for calculating MSE\n",
        "    error = []\n",
        "    diff = y_test.shape[0] - prediction_len * pre_win_num\n",
        "    # The error was calculated by comparing the original test set with the de-normalized forecast data\n",
        "    for i in range(y_test_ori.shape[0] - diff):\n",
        "        error.append(y_test_ori[i,] - de_predicted[i])\n",
        "    # Calculate the MSE by error\n",
        "    squaredError = []\n",
        "    for val in error:\n",
        "        squaredError.append(val * val)\n",
        "\n",
        "    MSE = sum(squaredError) / len(squaredError)\n",
        "    print(\"*****************************************************************************************************\")\n",
        "    print(\"LSTM-MSE:{}\".format(MSE))\n",
        "    print(\"*****************************************************************************************************\")"
      ],
      "metadata": {
        "id": "bTKvHDCOKyjt"
      },
      "id": "bTKvHDCOKyjt",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 1 Compare the effect of lstm and random forest prediction\n",
        "randomForest()\n",
        "singleInputLSTM()\n",
        "#Conclusion 1: LSTM is stronger than random forest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXKOsFMuFE1j",
        "outputId": "07bf9eaf-2be9-47a2-b424-dc75a3c00e92"
      },
      "id": "cXKOsFMuFE1j",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************************************\n",
            "Starting date:  2017-12-07 00:00:00\n",
            "Ending date:  2018-06-01 00:00:00\n",
            "Duration:  176 days 00:00:00\n",
            "Random forest MSE:  2420.2892448760804\n",
            "*******************************************************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 1/5\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.5261\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4881\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4533\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4203\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.3882\n",
            "2/2 [==============================] - 1s 11ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "*****************************************************************************************************\n",
            "LSTM-MSE:[2259.61468784]\n",
            "*****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime as dt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
        "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "def gridSearch():\n",
        "    # Import dataset\n",
        "    #The code reads a csv file named \"SP500.csv\" using pandas and stores it in a dataframe called \"bist100\".\n",
        "    bist100 = pd.read_csv(\"/content/drive/MyDrive/dp/SP500.csv\")\n",
        "    # Rename columns\n",
        "    bist100.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\"},\n",
        "                   inplace=True)\n",
        "    # Checking null value\n",
        "    bist100.isnull().sum()\n",
        "    # Checking na value\n",
        "    bist100.isna().any()\n",
        "    # The code drops any rows with null values using the \"dropna()\" method .\n",
        "    bist100.dropna(inplace=True)\n",
        "    #The code is used to check if there are any na values in the dataframe after dropping the null values.\n",
        "    bist100.isna().any()\n",
        "    # convert date field from string to Date format and make it index\n",
        "    bist100['date'] = pd.to_datetime(bist100.date)\n",
        "    # sorting dataset by date format\n",
        "    bist100.sort_values(by='date', inplace=True)\n",
        "    # Get the duration of dataset\n",
        "    # The time of the first bar of data\n",
        "\n",
        "    # Monthwise High and Low stock price\n",
        "    bist100.groupby(bist100['date'].dt.strftime('%B'))['low'].min()\n",
        "    #Keep close date data\n",
        "    closedf = bist100[['date', 'close']]\n",
        "    #Make a copy of the data for easy use\n",
        "    close_stock = closedf.copy()\n",
        "    #Delete date, leaving only close\n",
        "    del closedf['date']\n",
        "    # Maximum minimization normalization\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    closedf = scaler.fit_transform(np.array(closedf))\n",
        "    # Training data 0.65 Test data 0.35\n",
        "    \n",
        "    training_size = int(len(closedf) * 0.65)\n",
        "    test_size = len(closedf) - training_size\n",
        "    # Divide the data set according to the index divided above\n",
        "    train_data, test_data = closedf[0:test_size, :], closedf[test_size:len(closedf), :]\n",
        "    # Divide the data set according to the time window\n",
        "    # Using two weeks' worth of data to predict one day's worth of data\n",
        "    def create_dataset(dataset, time_step=1):\n",
        "        dataX, dataY = [], []\n",
        "        for i in range(len(dataset) - time_step - 1):\n",
        "            a = dataset[i:(i + time_step), 0]  ###i=0, 0,1,2,3------15\n",
        "            dataX.append(a)\n",
        "            dataY.append(dataset[i + time_step, 0])\n",
        "        return np.array(dataX), np.array(dataY)\n",
        "\n",
        "    # Using two weeks' worth of data to predict one day's worth of data\n",
        "    time_step = 10\n",
        "    X_train, y_train = create_dataset(train_data, time_step)\n",
        "    X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "############################################Use grid search to find the best parameters###############################################################################################################\n",
        "    #Iterate through all parameter values in a traversal manner\n",
        "    param_test1 = [{'n_estimators':[50,120,160,200,250]},{'max_depth':[1,2,3,5,7,9,11,13]},{'min_samples_split':[100,120,150,180,200,300]}]\n",
        "    #Create a grid search\n",
        "    gsearch1 = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_test1, cv=5)\n",
        "    #Use model fit data sets\n",
        "    gsearch1.fit(X_train,y_train)\n",
        "    print(\"*******************************************************************************************\")\n",
        "    print( \"By gridSearch The best model is :\",gsearch1.best_estimator_)#gridsearch.cv_results_Print the fitting results)\n",
        "    print(\"*******************************************************************************************\\n\\n\\n\\n\")\n",
        "##########################################################The grid search is complete###############################################################################################################"
      ],
      "metadata": {
        "id": "URH1pOWlhqLE"
      },
      "id": "URH1pOWlhqLE",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Opt_randomForest():\n",
        "    # Import dataset\n",
        "    #The code reads a csv file named \"SP500.csv\" using pandas and stores it in a dataframe called \"bist100\".\n",
        "    bist100 = pd.read_csv(\"/content/drive/MyDrive/dp/SP500.csv\")\n",
        "    # Rename columns\n",
        "    bist100.rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\"},\n",
        "                   inplace=True)\n",
        "    # Checking null value\n",
        "    bist100.isnull().sum()\n",
        "    # Checking na value\n",
        "    bist100.isna().any()\n",
        "    # The code drops any rows with null values using the \"dropna()\" method .\n",
        "    bist100.dropna(inplace=True)\n",
        "    #The code is used to check if there are any na values in the dataframe after dropping the null values.\n",
        "    bist100.isna().any()\n",
        "    # convert date field from string to Date format and make it index\n",
        "    bist100['date'] = pd.to_datetime(bist100.date)\n",
        "    # sorting dataset by date format\n",
        "    bist100.sort_values(by='date', inplace=True)\n",
        "    # Get the duration of dataset\n",
        "    # The time of the first bar of data\n",
        "    print(\"*******************************************************************************************\")\n",
        "    print(\"Starting date: \", bist100.iloc[0][0])\n",
        "    # Time of the last piece of data\n",
        "    print(\"Ending date: \", bist100.iloc[-1][0])\n",
        "    #duration\n",
        "    print(\"Duration: \", bist100.iloc[-1][0] - bist100.iloc[0][0])\n",
        "    # Monthwise High and Low stock price\n",
        "    bist100.groupby(bist100['date'].dt.strftime('%B'))['low'].min()\n",
        "    #Keep close date data\n",
        "    closedf = bist100[['date', 'close']]\n",
        "    #Make a copy of the data for easy use\n",
        "    close_stock = closedf.copy()\n",
        "    #Delete date, leaving only close\n",
        "    del closedf['date']\n",
        "    # Maximum minimization normalization\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    closedf = scaler.fit_transform(np.array(closedf))\n",
        "    # Training data 0.65 Test data 0.35\n",
        "    training_size = int(len(closedf) * 0.65)\n",
        "    test_size = len(closedf) - training_size\n",
        "    # Divide the data set according to the index divided above\n",
        "    train_data, test_data = closedf[0:test_size, :], closedf[test_size:len(closedf), :]\n",
        "    # Divide the data set according to the time window\n",
        "    # Using two weeks' worth of data to predict one day's worth of data\n",
        "    def create_dataset(dataset, time_step=5):\n",
        "        dataX, dataY = [], []\n",
        "        for i in range(len(dataset) - time_step - 1):\n",
        "            a = dataset[i:(i + time_step), 0]  ###i=0, 0,1,2,3------15\n",
        "            dataX.append(a)\n",
        "            dataY.append(dataset[i + time_step, 0])\n",
        "        return np.array(dataX), np.array(dataY)\n",
        "\n",
        "    # Using two weeks' worth of data to predict one day's worth of data\n",
        "    time_step = 10\n",
        "    X_train, y_train = create_dataset(train_data, time_step)\n",
        "    X_test, y_test = create_dataset(test_data, time_step)\n",
        "\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    # Build a random forest model\n",
        "    regressor = RandomForestRegressor(max_depth=13)\n",
        "    # Training model\n",
        "    regressor.fit(X_train, y_train)\n",
        "    # Lets Do the prediction\n",
        "    train_predict = regressor.predict(X_train)\n",
        "    test_predict = regressor.predict(X_test)\n",
        "    train_predict = train_predict.reshape(-1, 1)\n",
        "    test_predict = test_predict.reshape(-1, 1)\n",
        "    # From maximum to minimum normalization to its original form\n",
        "    train_predict = scaler.inverse_transform(train_predict)\n",
        "    test_predict = scaler.inverse_transform(test_predict)\n",
        "    original_ytrain = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
        "    original_ytest = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "    # Evaluation metrices MSE\n",
        "    print(\"Random forest algorithm after grid search optimization MSE: \", mean_squared_error(original_ytest, test_predict))\n",
        "    print(\"*******************************************************************************************\\n\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "RLP0aKVlS0KR"
      },
      "id": "RLP0aKVlS0KR",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gridSearch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfaB3qFzXPMi",
        "outputId": "7004b667-8d3b-4834-dd22-ab1160631833"
      },
      "id": "EfaB3qFzXPMi",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************************************\n",
            "By gridSearch The best model is : RandomForestRegressor(max_depth=13)\n",
            "*******************************************************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a2ccdff5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "a2ccdff5",
        "outputId": "57af7383-258b-465c-bc6c-64e7fe176053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************************************\n",
            "Starting date:  2017-12-07 00:00:00\n",
            "Ending date:  2018-06-01 00:00:00\n",
            "Duration:  176 days 00:00:00\n",
            "Random forest MSE:  2536.899516624479\n",
            "*******************************************************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "*******************************************************************************************\n",
            "Starting date:  2017-12-07 00:00:00\n",
            "Ending date:  2018-06-01 00:00:00\n",
            "Duration:  176 days 00:00:00\n",
            "Random forest algorithm after grid search optimization MSE:  1684.5183216721784\n",
            "*******************************************************************************************\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Common stochastic forest models predict performance\n",
        "randomForest()\n",
        "\n",
        "#Test the predictive performance of the best model, when(max_depth=11)\n",
        "Opt_randomForest()\n",
        "\n",
        "#The second conclusion is that gridded search can help provide the effect of machine learning model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "import statsmodels.api as sm \n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats\n",
        "#In order to reduce the interference of false information and increase the robustness of data, Gaussian noise is added as a training set\n",
        "def add_noise():\n",
        "  #Read stock price information with emotional information assignment\n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "\n",
        "  #Calculate the variance of the sentiment analysis value for each company\n",
        "  wsj_var=np.var(df.wsj_mean_compound)\n",
        "  cnbc_var=np.var(df.cnbc_mean_compound)\n",
        "  fortune_var=np.var(df.fortune_mean_compound)\n",
        "  reuters_var=np.var(df.reuters_mean_compound)\n",
        "\n",
        "  # The mean of the initialization probability distribution is 0\n",
        "  mu=0\n",
        "  # Calculate the standard deviation of the probability distribution of sentiment analysis values for each company\n",
        "  sigma_wsj=0.1*wsj_var\n",
        "  sigma_cnbc=0.1*cnbc_var\n",
        "  sigma_fortune=0.1*fortune_var\n",
        "  sigma_reuters=0.1*reuters_var\n",
        "  # shape[0] means the number of rows in the matrix\n",
        "  n=df.shape[0]\n",
        "  # Create a new table\n",
        "  df_noise=pd.DataFrame()\n",
        "  # Specify the source of data for each column\n",
        "  df_noise['wsj_noise']=df['wsj_mean_compound']\n",
        "  df_noise['cnbc_noise']=df['cnbc_mean_compound']\n",
        "  df_noise['fortune_noise']=df['fortune_mean_compound']\n",
        "  df_noise['reuters_noise']=df['reuters_mean_compound']\n",
        "  # Add noise to each column of data\n",
        "  for i in range(0,n):\n",
        "    df_noise['wsj_noise'][i]+=np.random.normal(mu,sigma_wsj)\n",
        "    df_noise['cnbc_noise'][i]+=np.random.normal(mu,sigma_cnbc)\n",
        "    df_noise['fortune_noise'][i]+=np.random.normal(mu,sigma_fortune)\n",
        "    df_noise['reuters_noise'][i]+=np.random.normal(mu,sigma_reuters)\n",
        "  # Save the data you just processed to a file\n",
        "  df_noise.to_csv(\"/content/drive/MyDrive/dp/source_price_noise.csv\")\n",
        "  print(\"*****************************************************************************************************\")\n",
        "  print(\"Gaussian noise was successfully added to the data set\")\n",
        "  print(\"*****************************************************************************************************\")"
      ],
      "metadata": {
        "id": "wcW6nT1Xqs79"
      },
      "id": "wcW6nT1Xqs79",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats\n",
        "def twoWeekSentimentLSTM():#Training set: Gaussian noise was added to each company, and four tables were aggregated together as the training set test set: the original stock price with sentiment analysis without noise added\n",
        "    # Read and process generated noise data and raw data\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "    dfn = pd.read_csv(\"/content/drive/MyDrive/dp/source_price_noise.csv\", index_col=0)\n",
        "\n",
        "    # Create a data table for each company. Create four tables in total\n",
        "\n",
        "    # df_1n indicates adding noise to wsj\n",
        "    df_1n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_1n['wsj'] = dfn['wsj_noise']\n",
        "    df_1n['cnbc'] = df['cnbc_mean_compound']\n",
        "    df_1n['fortune'] = df['fortune_mean_compound']\n",
        "    df_1n['reuters'] = df['reuters_mean_compound']\n",
        "    df_1n['price'] = df['Adj Close']\n",
        "\n",
        "    # df_2n represents the cnbc company to increase noise\n",
        "    df_2n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_2n['wsj'] = df['wsj_mean_compound']\n",
        "    df_2n['cnbc'] = dfn['cnbc_noise']\n",
        "    df_2n['fortune'] = df['fortune_mean_compound']\n",
        "    df_2n['reuters'] = df['reuters_mean_compound']\n",
        "    df_2n['price'] = df['Adj Close']\n",
        "\n",
        "    # df_3n represents fortune to increase noise\n",
        "    df_3n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_3n['wsj'] = df['wsj_mean_compound']\n",
        "    df_3n['cnbc'] = df['cnbc_mean_compound']\n",
        "    df_3n['fortune'] = dfn['fortune_noise']\n",
        "    df_3n['reuters'] = df['reuters_mean_compound']\n",
        "    df_3n['price'] = df['Adj Close']\n",
        "\n",
        "    # df_4n represents the reuters company to increase noise\n",
        "    df_4n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_4n['wsj'] = df['wsj_mean_compound']\n",
        "    df_4n['cnbc'] = df['cnbc_mean_compound']\n",
        "    df_4n['fortune'] = df['fortune_mean_compound']\n",
        "    df_4n['reuters'] = dfn['reuters_noise']\n",
        "    df_4n['price'] = df['Adj Close']\n",
        "\n",
        "    # Copy data table\n",
        "    df1 = df_1n\n",
        "    df2 = df_2n\n",
        "    df3 = df_3n\n",
        "    df4 = df_4n\n",
        "\n",
        "    # The training set size is 0.65\n",
        "    split = (0.65)\n",
        "    # The predicted sequence length is 10\n",
        "    sequence_length = 10;\n",
        "    # Data normalization\n",
        "    normalise = True\n",
        "    # Design the batch size of the neural network\n",
        "    batch_size = 100;\n",
        "    # The dimension of the input data is 5\n",
        "    input_dim = 5\n",
        "    # The length of input prediction data is 9, which means that the first 9 data are used to predict the 10th data\n",
        "    input_timesteps = 9\n",
        "    # Set the number of neurons for the neural network to 50\n",
        "    neurons = 50\n",
        "    # As the number of epochs increases, the number of weight update iterations increases, and the curve shifts from the initial unfitting state to the optimized fitting state\n",
        "    epochs = 5\n",
        "    # The number of predicted data at a time\n",
        "    prediction_len = 1\n",
        "    # dense layer Number of output data\n",
        "    dense_output = 1\n",
        "    # Sets the probability of data returning to zero\n",
        "    drop_out = 0\n",
        "    # Calculate the size of the partition training set\n",
        "    i_split = int(len(df1) * split)\n",
        "    # Select the columns to use\n",
        "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
        "    # Partition training data\n",
        "    data_train_1 = df1.get(cols).values[:i_split]\n",
        "    data_train_2 = df2.get(cols).values[:i_split]\n",
        "    data_train_3 = df3.get(cols).values[:i_split]\n",
        "    data_train_4 = df4.get(cols).values[:i_split]\n",
        "    # Gets the data set length\n",
        "    len_train = len(data_train_1)\n",
        "    len_train_windows = None\n",
        "    # Adds a training set to the sliding window\n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_1[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    # Set the parameters of the sliding window\n",
        "    window_data = data_windows\n",
        "    # Sets the number of sliding Windows\n",
        "    win_num = window_data.shape[0]\n",
        "    # Sets the number of columns\n",
        "    col_num = window_data.shape[2]\n",
        "    # Create the data container needed to record the maximum and minimum normalized data\n",
        "    normalised_data = []\n",
        "    # Record minimum values for calculation purposes\n",
        "    record_min = []\n",
        "    # Record the maximum value for calculation\n",
        "    record_max = []\n",
        "    # Carry out maximum minimization normalization\n",
        "    for win_i in range(0, win_num):# Each sliding window is normalized\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        #Normalized data was recorded\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    # Get wsj company added noise training set\n",
        "    x_train1 = data_windows[:, :-1]\n",
        "    y_train1 = data_windows[:, -1, [0]]\n",
        "\n",
        "    ##################################################################################################The following is repeated with the ordinary LSTM######################################################################################################################\n",
        "    # get_train_data \n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_2[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    x_train2 = data_windows[:, :-1]\n",
        "    y_train2 = data_windows[:, -1, [0]]\n",
        "\n",
        "    # get_train_data \n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_3[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    x_train3 = data_windows[:, :-1]\n",
        "    y_train3 = data_windows[:, -1, [0]]\n",
        "\n",
        "    # get_train_data \n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_4[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    x_train4 = data_windows[:, :-1]\n",
        "    y_train4 = data_windows[:, -1, [0]]\n",
        "\n",
        "    # Put four data tables together\n",
        "    x_train_t = np.concatenate((x_train1, x_train2, x_train3, x_train4), axis=0)\n",
        "    # Copy the data for subsequent operations\n",
        "    x_train = x_train_t\n",
        "    # Put four data tables together\n",
        "    y_train_t = np.concatenate((y_train1, y_train2, y_train3, y_train4), axis=0)\n",
        "    # Copy the data for subsequent operations\n",
        "    y_train = y_train_t\n",
        "\n",
        "    # Read the noiseless data for prediction\n",
        "    dataframe = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "    # According to the data in these six columns\n",
        "    dataframe.columns = ['date', 'wsj', 'cnbc', 'fortune', 'reuters', 'price']\n",
        "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
        "    # Gets the number of rows in the table\n",
        "    len_dataframe = dataframe.shape[0]\n",
        "    # Gets the size of the test set and divides the data set\n",
        "    i_split = int(len(dataframe) * split)\n",
        "    data_test = dataframe.get(cols).values[i_split:]\n",
        "    # Calculates the length of the test set\n",
        "    len_test = len(data_test)\n",
        "    len_train_windows = None\n",
        "\n",
        "    # Add a test set to the sliding window\n",
        "    data_windows = []\n",
        "    for i in range(len_test - sequence_length):\n",
        "        data_windows.append(data_test[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    # Save the initial test set for the final MSE calculation\n",
        "    y_test_ori = data_windows[:, -1, [0]]\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "\n",
        "    # normalize\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data  # get_test_data\n",
        "    x_test = data_windows[:, :-1]\n",
        "    y_test = data_windows[:, -1, [0]]\n",
        "\n",
        "    # Create the LSTM model\n",
        "    model = Sequential()\n",
        "    # Build the LSTM hierarchy\n",
        "    model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=True))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(LSTM(neurons, return_sequences=True))\n",
        "    model.add(LSTM(neurons, return_sequences=False))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(Dense(dense_output, activation='linear'))\n",
        "    # Compile model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer='adam')\n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    # Initializes the container and parameter structure used by the prediction\n",
        "    data = x_test\n",
        "    prediction_seqs = []\n",
        "    window_size = sequence_length\n",
        "    pre_win_num = int(len(data) / prediction_len)\n",
        "    #multi sequence predict\n",
        "    for i in range(0, pre_win_num):#For each sliding window, make a prediction and add the prediction results to the next sliding window\n",
        "        curr_frame = data[i * prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(0, prediction_len):\n",
        "            temp = model.predict(curr_frame[newaxis, :, :])[0]\n",
        "            # Record the predicted results\n",
        "            predicted.append(temp)\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "\n",
        "    # Initializes the container and parameters used for de-normalization\n",
        "    de_predicted = []\n",
        "    len_pre_win = int(len(data) / prediction_len)\n",
        "    len_pre = prediction_len\n",
        "    # De-normalize the predicted data, that is, remove the maximum and minimum value normalization\n",
        "    m = 0\n",
        "    for i in range(0, len_pre_win):\n",
        "        for j in range(0, len_pre):\n",
        "            de_predicted.append(prediction_seqs[i][j][0] * record_max[m] + record_min[m])\n",
        "            m = m + 1\n",
        "    # Initialize the container and parameters for calculating MSE\n",
        "    error = []\n",
        "    diff = y_test.shape[0] - prediction_len * pre_win_num\n",
        "    # The error was calculated by comparing the original test set with the de-normalized forecast data\n",
        "    for i in range(y_test_ori.shape[0] - diff):\n",
        "        error.append(y_test_ori[i,] - de_predicted[i])\n",
        "    # Calculate the MSE by error\n",
        "    squaredError = []\n",
        "    absError = []\n",
        "    for val in error:\n",
        "        squaredError.append(val * val)\n",
        "\n",
        "    MSE = sum(squaredError) / len(squaredError)\n",
        "    print(\"*****************************************************************************************************\")\n",
        "    print(\"Sentimental-LSTM MSE:{}\".format(MSE))\n",
        "    print(\"*****************************************************************************************************\")"
      ],
      "metadata": {
        "id": "ruFAecIxoGIN"
      },
      "id": "ruFAecIxoGIN",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9ny2oy5gWxv",
        "outputId": "5568f13f-c4ab-466f-9e0f-dd551c5948c2"
      },
      "id": "h9ny2oy5gWxv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dd = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "dd.head()\n",
        "#For data sets with sentiment analysis, positive values represent positive evaluations and negative values represent negative evaluations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UNic2EWYIWfk",
        "outputId": "47bbdde3-89cb-472b-b1d3-cef98d54e01a"
      },
      "id": "UNic2EWYIWfk",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date  wsj_mean_compound  cnbc_mean_compound  fortune_mean_compound  \\\n",
              "0   2017/12/7              0.296             -0.1366                 0.0000   \n",
              "1   2017/12/8              0.000              0.0000                -0.2423   \n",
              "2  2017/12/11              0.000              0.0000                 0.0000   \n",
              "3  2017/12/12              0.000              0.0000                 0.0000   \n",
              "4  2017/12/13              0.000              0.0000                 0.0000   \n",
              "\n",
              "   reuters_mean_compound    Adj Close  \n",
              "0                    0.0  2636.979980  \n",
              "1                    0.0  2651.500000  \n",
              "2                    0.0  2659.989990  \n",
              "3                    0.0  2664.110107  \n",
              "4                    0.0  2662.850098  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d19fbd82-ce15-48e3-85dc-236f1bb500cd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>wsj_mean_compound</th>\n",
              "      <th>cnbc_mean_compound</th>\n",
              "      <th>fortune_mean_compound</th>\n",
              "      <th>reuters_mean_compound</th>\n",
              "      <th>Adj Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017/12/7</td>\n",
              "      <td>0.296</td>\n",
              "      <td>-0.1366</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2636.979980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017/12/8</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.2423</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2651.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017/12/11</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2659.989990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017/12/12</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2664.110107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017/12/13</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2662.850098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d19fbd82-ce15-48e3-85dc-236f1bb500cd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d19fbd82-ce15-48e3-85dc-236f1bb500cd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d19fbd82-ce15-48e3-85dc-236f1bb500cd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3 The optimized LSTM based on Sentiment analysis The optimized LSTM based on the stock price data with sentiment analysis of four financial news\n",
        "add_noise()# Gaussian noise is added to the data set to reduce the interference of fake news as a training set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNHXBGRVG419",
        "outputId": "c481f203-46fe-497a-c802-4c4f2bc6ab1a"
      },
      "id": "VNHXBGRVG419",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************************************************************************************************\n",
            "Gaussian noise was successfully added to the data set\n",
            "*****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "singleInputLSTM()#Basic LSTM model\n",
        "twoWeekSentimentLSTM()#LSTM model based on sentiment analysis\n",
        "# Conclusion 3: The optimization of LSTM based on sentiment analysis can improve the prediction accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epmgDzVNH9Y8",
        "outputId": "9f320529-6f4d-4c7b-b7ff-47bef27fb5a1"
      },
      "id": "epmgDzVNH9Y8",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.4699\n",
            "Epoch 2/5\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4347\n",
            "Epoch 3/5\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4000\n",
            "Epoch 4/5\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.3654\n",
            "Epoch 5/5\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.3307\n",
            "2/2 [==============================] - 1s 9ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "*****************************************************************************************************\n",
            "LSTM-MSE:[1823.25330162]\n",
            "*****************************************************************************************************\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 8s 25ms/step - loss: 0.4767\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3772\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2772\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2281\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.2255\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "*****************************************************************************************************\n",
            "Sentimental-LSTM MSE:[829.96458568]\n",
            "*****************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from numpy import newaxis\n",
        "from keras.layers import Dense, Activation, Dropout, LSTM\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from math import pi,sqrt,exp,pow,log\n",
        "from numpy.linalg import det, inv\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from sklearn import cluster\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scs\n",
        "import scipy.optimize as sco\n",
        "import scipy.interpolate as sci\n",
        "from scipy import stats\n",
        "def oneWeekSentimentLSTM():#Training set: Gaussian noise was added to each company, and four tables were aggregated together as the training set test set: the original stock price with sentiment analysis without noise added\n",
        "    # Read and process generated noise data and raw data\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "    dfn = pd.read_csv(\"/content/drive/MyDrive/dp/source_price_noise.csv\", index_col=0)\n",
        "\n",
        "    # Create a data table for each company\n",
        "\n",
        "    # df_1n indicates adding noise to wsj\n",
        "    df_1n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_1n['wsj'] = dfn['wsj_noise']\n",
        "    df_1n['cnbc'] = df['cnbc_mean_compound']\n",
        "    df_1n['fortune'] = df['fortune_mean_compound']\n",
        "    df_1n['reuters'] = df['reuters_mean_compound']\n",
        "    df_1n['price'] = df['Adj Close']\n",
        "\n",
        "    # df_2n represents the cnbc company to increase noise\n",
        "    df_2n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_2n['wsj'] = df['wsj_mean_compound']\n",
        "    df_2n['cnbc'] = dfn['cnbc_noise']\n",
        "    df_2n['fortune'] = df['fortune_mean_compound']\n",
        "    df_2n['reuters'] = df['reuters_mean_compound']\n",
        "    df_2n['price'] = df['Adj Close']\n",
        "\n",
        "    # df_3n represents fortune to increase noise\n",
        "    df_3n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_3n['wsj'] = df['wsj_mean_compound']\n",
        "    df_3n['cnbc'] = df['cnbc_mean_compound']\n",
        "    df_3n['fortune'] = dfn['fortune_noise']\n",
        "    df_3n['reuters'] = df['reuters_mean_compound']\n",
        "    df_3n['price'] = df['Adj Close']\n",
        "\n",
        "    # df_4n represents the reuters company to increase noise\n",
        "    df_4n = pd.DataFrame()\n",
        "    # Specifies the source of data for each column of the data table\n",
        "    df_4n['wsj'] = df['wsj_mean_compound']\n",
        "    df_4n['cnbc'] = df['cnbc_mean_compound']\n",
        "    df_4n['fortune'] = df['fortune_mean_compound']\n",
        "    df_4n['reuters'] = dfn['reuters_noise']\n",
        "    df_4n['price'] = df['Adj Close']\n",
        "\n",
        "    # Copy data table\n",
        "    df1 = df_1n\n",
        "    df2 = df_2n\n",
        "    df3 = df_3n\n",
        "    df4 = df_4n\n",
        "\n",
        "    # The training set size is 0.65\n",
        "    split = (0.65)\n",
        "    ##########################################################################sequence_length changed to 5 and input_timesteps changed to 4, changing to a sliding window of one week, using the first four data to predict the fifth data#######################################################################################\n",
        "    # The predicted sequence length is 5\n",
        "    sequence_length = 5;\n",
        "    # Data normalization\n",
        "    normalise = True\n",
        "    # Design the batch size of the neural network\n",
        "    batch_size = 100;\n",
        "    # The dimension of the input data is 5\n",
        "    input_dim = 5\n",
        "    # The length of input prediction data is 4, which means that the first 4 data are used to predict the fifth data\n",
        "    input_timesteps = 4\n",
        "    ############################################################################################################################################################################################################################################################\n",
        "    # Set the number of neurons for the neural network to 50\n",
        "    neurons = 50\n",
        "    # As the number of epochs increases, the number of weight update iterations increases, and the curve shifts from the initial unfitting state to the optimized fitting state\n",
        "    epochs = 5\n",
        "    # The number of predicted data at a time\n",
        "    prediction_len = 1\n",
        "    # dense layer Number of output data\n",
        "    dense_output = 1\n",
        "    # Sets the probability of data returning to zero\n",
        "    drop_out = 0\n",
        "\n",
        "    # Calculate the size of the partition training set\n",
        "    i_split = int(len(df1) * split)\n",
        "    # Select the columns to use\n",
        "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
        "    # Partition training data\n",
        "    data_train_1 = df1.get(cols).values[:i_split]\n",
        "    data_train_2 = df2.get(cols).values[:i_split]\n",
        "    data_train_3 = df3.get(cols).values[:i_split]\n",
        "    data_train_4 = df4.get(cols).values[:i_split]\n",
        "    # Gets the data set length\n",
        "    len_train = len(data_train_1)\n",
        "    len_train_windows = None\n",
        "    # Adds a training set to the sliding window\n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_1[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    # Set the parameters of the sliding window\n",
        "    window_data = data_windows\n",
        "    # Sets the number of sliding Windows\n",
        "    win_num = window_data.shape[0]\n",
        "    # Sets the number of columns\n",
        "    col_num = window_data.shape[2]\n",
        "    # Create the data container needed to record the maximum and minimum normalized data\n",
        "    normalised_data = []\n",
        "    # Record minimum values for calculation purposes\n",
        "    record_min = []\n",
        "    # Record the maximum value for calculation\n",
        "    record_max = []\n",
        "    # Carry out maximum minimization normalization\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    # Get wsj company added noise training set\n",
        "    x_train1 = data_windows[:, :-1]\n",
        "    y_train1 = data_windows[:, -1, [0]]\n",
        "\n",
        "\n",
        "    # get_train_data \n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_2[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    x_train2 = data_windows[:, :-1]\n",
        "    y_train2 = data_windows[:, -1, [0]]\n",
        "\n",
        "    # get_train_data \n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_3[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    x_train3 = data_windows[:, :-1]\n",
        "    y_train3 = data_windows[:, -1, [0]]\n",
        "\n",
        "    # get_train_data \n",
        "    data_windows = []\n",
        "    for i in range(len_train - sequence_length):\n",
        "        data_windows.append(data_train_4[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data\n",
        "    x_train4 = data_windows[:, :-1]\n",
        "    y_train4 = data_windows[:, -1, [0]]\n",
        "\n",
        "    # Put four data tables together\n",
        "    x_train_t = np.concatenate((x_train1, x_train2, x_train3, x_train4), axis=0)\n",
        "    # Copy the data for subsequent operations\n",
        "    x_train = x_train_t\n",
        "    # Put four data tables together\n",
        "    y_train_t = np.concatenate((y_train1, y_train2, y_train3, y_train4), axis=0)\n",
        "    # Copy the data for subsequent operations\n",
        "    y_train = y_train_t\n",
        "\n",
        "    # Read the noiseless data for prediction\n",
        "    dataframe = pd.read_csv(\"/content/drive/MyDrive/dp/source_price.csv\")\n",
        "    # According to the data in these six columns\n",
        "    dataframe.columns = ['date', 'wsj', 'cnbc', 'fortune', 'reuters', 'price']\n",
        "    cols = ['price', 'wsj', 'cnbc', 'fortune', 'reuters']\n",
        "    # Gets the number of rows in the table\n",
        "    len_dataframe = dataframe.shape[0]\n",
        "    # Gets the size of the test set and divides the data set\n",
        "    i_split = int(len(dataframe) * split)\n",
        "    data_test = dataframe.get(cols).values[i_split:]\n",
        "    # Calculates the length of the test set\n",
        "    len_test = len(data_test)\n",
        "    len_train_windows = None\n",
        "\n",
        "    # Add a test set to the sliding window\n",
        "    data_windows = []\n",
        "    for i in range(len_test - sequence_length):\n",
        "        data_windows.append(data_test[i:i + sequence_length])\n",
        "    data_windows = np.array(data_windows).astype(float)\n",
        "    # Save the initial test set for the final MSE calculation\n",
        "    y_test_ori = data_windows[:, -1, [0]]\n",
        "\n",
        "    window_data = data_windows\n",
        "    win_num = window_data.shape[0]\n",
        "    col_num = window_data.shape[2]\n",
        "    normalised_data = []\n",
        "    record_min = []\n",
        "    record_max = []\n",
        "\n",
        "    # normalize\n",
        "    for win_i in range(0, win_num):\n",
        "        normalised_window = []\n",
        "        for col_i in range(0, 1):  # col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            temp_min = min(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_min.append(temp_min)  # record min\n",
        "            temp_col = temp_col - temp_min\n",
        "            temp_max = max(temp_col)\n",
        "            if col_i == 0:\n",
        "                record_max.append(temp_max)  # record max\n",
        "            temp_col = temp_col / temp_max\n",
        "            normalised_window.append(temp_col)\n",
        "        for col_i in range(1, col_num):\n",
        "            temp_col = window_data[win_i, :, col_i]\n",
        "            normalised_window.append(temp_col)\n",
        "        normalised_window = np.array(normalised_window).T\n",
        "        normalised_data.append(normalised_window)\n",
        "    normalised_data = np.array(normalised_data)\n",
        "\n",
        "    # normalised_data=window_data\n",
        "    data_windows = normalised_data  # get_test_data\n",
        "    x_test = data_windows[:, :-1]\n",
        "    y_test = data_windows[:, -1, [0]]\n",
        "    # Create the LSTM model\n",
        "    model = Sequential()\n",
        "    # Build the LSTM hierarchy\n",
        "    model.add(LSTM(neurons, input_shape=(input_timesteps, input_dim), return_sequences=True))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(LSTM(neurons, return_sequences=True))\n",
        "    model.add(LSTM(neurons, return_sequences=False))\n",
        "    model.add(Dropout(drop_out))\n",
        "    model.add(Dense(dense_output, activation='linear'))\n",
        "    # Compile model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer='adam')\n",
        "    # Fit the model\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    # Initializes the container and parameters used by the prediction\n",
        "    data = x_test\n",
        "    prediction_seqs = []\n",
        "    window_size = sequence_length\n",
        "    pre_win_num = int(len(data) / prediction_len)\n",
        "    #multi sequence predict\n",
        "    for i in range(0, pre_win_num):#For each sliding window, make a prediction and add the prediction results to the next sliding window\n",
        "        curr_frame = data[i * prediction_len]\n",
        "        predicted = []\n",
        "        for j in range(0, prediction_len):\n",
        "            temp = model.predict(curr_frame[newaxis, :, :])[0]\n",
        "            # Record the predicted results and add them to the next slide window\n",
        "            predicted.append(temp)\n",
        "            curr_frame = curr_frame[1:]\n",
        "            curr_frame = np.insert(curr_frame, [window_size - 2], predicted[-1], axis=0)\n",
        "        prediction_seqs.append(predicted)\n",
        "\n",
        "    # Initializes the container and parameters used for de-normalization\n",
        "    de_predicted = []\n",
        "    len_pre_win = int(len(data) / prediction_len)\n",
        "    len_pre = prediction_len\n",
        "    # De-normalize the predicted data, that is, remove the maximum and minimum value normalization\n",
        "    m = 0\n",
        "    for i in range(0, len_pre_win):\n",
        "        for j in range(0, len_pre):\n",
        "            de_predicted.append(prediction_seqs[i][j][0] * record_max[m] + record_min[m])\n",
        "            m = m + 1\n",
        "    # Initialize the container and parameters for calculating MSE\n",
        "    error = []\n",
        "    diff = y_test.shape[0] - prediction_len * pre_win_num\n",
        "    # The error was calculated by comparing the original test set with the de-normalized forecast data\n",
        "    for i in range(y_test_ori.shape[0] - diff):\n",
        "        error.append(y_test_ori[i,] - de_predicted[i])\n",
        "    # Calculate the MSE by error\n",
        "    squaredError = []\n",
        "    absError = []\n",
        "    for val in error:\n",
        "        squaredError.append(val * val)\n",
        "\n",
        "    MSE = sum(squaredError) / len(squaredError)\n",
        "    print(\"*****************************************************************************************************\")\n",
        "    print(\"one week Sentimental-LSTM MSE:{}\".format(MSE))\n",
        "    print(\"*****************************************************************************************************\")"
      ],
      "metadata": {
        "id": "Dv0yJY_lhtFr"
      },
      "id": "Dv0yJY_lhtFr",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "UeTJw7U7jwp6"
      },
      "id": "UeTJw7U7jwp6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 4 Change the sliding window from two weeks to one week to see the predicted effect\n",
        "twoWeekSentimentLSTM()\n",
        "oneWeekSentimentLSTM()\n",
        "# Conclusion 4, a week for sliding window can achieve better results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgmV_Hw5JO4e",
        "outputId": "67393a08-cc31-4949-ba33-5baf2998ca2e"
      },
      "id": "DgmV_Hw5JO4e",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3/3 [==============================] - 6s 40ms/step - loss: 0.4610\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.3647\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 38ms/step - loss: 0.2750\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 39ms/step - loss: 0.2325\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 43ms/step - loss: 0.2329\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "*****************************************************************************************************\n",
            "Sentimental-LSTM MSE:[864.14441897]\n",
            "*****************************************************************************************************\n",
            "Epoch 1/5\n",
            "3/3 [==============================] - 6s 24ms/step - loss: 0.4964\n",
            "Epoch 2/5\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4531\n",
            "Epoch 3/5\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.4070\n",
            "Epoch 4/5\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.3531\n",
            "Epoch 5/5\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3001\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "*****************************************************************************************************\n",
            "one week Sentimental-LSTM MSE:[582.97939972]\n",
            "*****************************************************************************************************\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensor1",
      "language": "python",
      "name": "tensor1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
